{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcf92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization:\n",
      "['Machine', 'learning', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '.', 'It', 'focuses', 'on', 'the', 'development', 'of', 'algorithms', 'and', 'models', 'that', 'enable', 'computers', 'to', 'learn', 'and', 'make', 'predictions', 'or', 'decisions', 'without', 'explicit', 'programming', '.', 'Machine', 'learning', 'techniques', 'have', 'found', 'applications', 'in', 'various', 'domains', ',', 'including', 'image', 'recognition', ',', 'natural', 'language', 'processing', ',', 'and', 'data', 'analysis', '.']\n",
      "\n",
      "POS Tagging:\n",
      "[('Machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('.', '.'), ('It', 'PRP'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('development', 'NN'), ('of', 'IN'), ('algorithms', 'NN'), ('and', 'CC'), ('models', 'NNS'), ('that', 'IN'), ('enable', 'JJ'), ('computers', 'NNS'), ('to', 'TO'), ('learn', 'VB'), ('and', 'CC'), ('make', 'VB'), ('predictions', 'NNS'), ('or', 'CC'), ('decisions', 'NNS'), ('without', 'IN'), ('explicit', 'JJ'), ('programming', 'NN'), ('.', '.'), ('Machine', 'NNP'), ('learning', 'VBG'), ('techniques', 'NNS'), ('have', 'VBP'), ('found', 'VBN'), ('applications', 'NNS'), ('in', 'IN'), ('various', 'JJ'), ('domains', 'NNS'), (',', ','), ('including', 'VBG'), ('image', 'NN'), ('recognition', 'NN'), (',', ','), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), (',', ','), ('and', 'CC'), ('data', 'NNS'), ('analysis', 'NN'), ('.', '.')]\n",
      "\n",
      "Stop Words Removal:\n",
      "['Machine', 'learning', 'subfield', 'artificial', 'intelligence', '.', 'focuses', 'development', 'algorithms', 'models', 'enable', 'computers', 'learn', 'make', 'predictions', 'decisions', 'without', 'explicit', 'programming', '.', 'Machine', 'learning', 'techniques', 'found', 'applications', 'various', 'domains', ',', 'including', 'image', 'recognition', ',', 'natural', 'language', 'processing', ',', 'data', 'analysis', '.']\n",
      "\n",
      "Stemming:\n",
      "['machin', 'learn', 'subfield', 'artifici', 'intellig', '.', 'focus', 'develop', 'algorithm', 'model', 'enabl', 'comput', 'learn', 'make', 'predict', 'decis', 'without', 'explicit', 'program', '.', 'machin', 'learn', 'techniqu', 'found', 'applic', 'variou', 'domain', ',', 'includ', 'imag', 'recognit', ',', 'natur', 'languag', 'process', ',', 'data', 'analysi', '.']\n",
      "\n",
      "Lemmatization:\n",
      "['Machine', 'learning', 'subfield', 'artificial', 'intelligence', '.', 'focus', 'development', 'algorithm', 'model', 'enable', 'computer', 'learn', 'make', 'prediction', 'decision', 'without', 'explicit', 'programming', '.', 'Machine', 'learning', 'technique', 'found', 'application', 'various', 'domain', ',', 'including', 'image', 'recognition', ',', 'natural', 'language', 'processing', ',', 'data', 'analysis', '.']\n",
      "\n",
      "TF-IDF Representation:\n",
      "algorithms: 0.13018891098082389\n",
      "analysis: 0.13018891098082389\n",
      "and: 0.39056673294247163\n",
      "applications: 0.13018891098082389\n",
      "artificial: 0.13018891098082389\n",
      "computers: 0.13018891098082389\n",
      "data: 0.13018891098082389\n",
      "decisions: 0.13018891098082389\n",
      "development: 0.13018891098082389\n",
      "domains: 0.13018891098082389\n",
      "enable: 0.13018891098082389\n",
      "explicit: 0.13018891098082389\n",
      "focuses: 0.13018891098082389\n",
      "found: 0.13018891098082389\n",
      "have: 0.13018891098082389\n",
      "image: 0.13018891098082389\n",
      "in: 0.13018891098082389\n",
      "including: 0.13018891098082389\n",
      "intelligence: 0.13018891098082389\n",
      "is: 0.13018891098082389\n",
      "it: 0.13018891098082389\n",
      "language: 0.13018891098082389\n",
      "learn: 0.13018891098082389\n",
      "learning: 0.26037782196164777\n",
      "machine: 0.26037782196164777\n",
      "make: 0.13018891098082389\n",
      "models: 0.13018891098082389\n",
      "natural: 0.13018891098082389\n",
      "of: 0.26037782196164777\n",
      "on: 0.13018891098082389\n",
      "or: 0.13018891098082389\n",
      "predictions: 0.13018891098082389\n",
      "processing: 0.13018891098082389\n",
      "programming: 0.13018891098082389\n",
      "recognition: 0.13018891098082389\n",
      "subfield: 0.13018891098082389\n",
      "techniques: 0.13018891098082389\n",
      "that: 0.13018891098082389\n",
      "the: 0.13018891098082389\n",
      "to: 0.13018891098082389\n",
      "various: 0.13018891098082389\n",
      "without: 0.13018891098082389\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample Document\n",
    "document = \"\"\"\n",
    "Machine learning is a subfield of artificial intelligence. It focuses on the development of algorithms and models that enable computers to learn and make predictions or decisions without explicit programming. Machine learning techniques have found applications in various domains, including image recognition, natural language processing, and data analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(document)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Stop words removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Tokenization:\")\n",
    "print(tokens)\n",
    "print(\"\\nPOS Tagging:\")\n",
    "print(pos_tags)\n",
    "print(\"\\nStop Words Removal:\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\nStemming:\")\n",
    "print(stemmed_tokens)\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([document])\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_representation = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print(f\"{feature}: {tfidf_representation[0][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfba8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ketul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ketul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ketul\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037e949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
